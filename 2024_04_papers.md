# Tutorial on Variational Autoencoders

## 1 Introduction

​	生成式模型是处理数据概率分布的模型，即$P(X)$的分布。

​		其中$X$是数据，其潜在高维空间是$\chi$。

​	一个最直观的模型就是：我们能够对$P(X)$进行数值计算，得到它的值，使更像图像的$X$概率更大。然而，这不一定是必要的，因为我们即使能算出来它的概率，也不能帮助我们知道如何生成一个概率更大的数据样本$X$。

​	可以替代的是，我们通过构建一个数据集，之后构建一个生成模型，使之生成与数据集中的图像很相似的图像的概率更大。我们可以把这一步骤抽象为：

​		1. 数据集的真实分布为$P_{gt}(X)$

​		2. 通过训练一个**可采样的**生成式模型$P$，使之尽量逼近$P_{gt}(X)$

​	训练这样一个模型往往依赖于三种手段：

​		1. 需要对数据记得结构做出很强的先验假设

​		2. 可能会做一些很严格的近似，使其转化为一些子优化问题

​		3. 可能使用强大算力来近似模拟，如Markov Chain Monte Carlo

​	现在，发展出了一些依赖于神经网络的手段。

## 1.1 Preliminaries: Latent Variable Models

​	当训练一个生成式模型的时候，不同维度的关联关系越复杂，模型越难以训练。以手写数字的图片为例，这种关联体现在，如果字符的左半边是5，那么字符的右半边不能是0，否则的话这个字符就不像是手写数字了。直观的讲，如果模型首先决定了生成的数字是几，那么它在生成图片的时候就会轻松的多。这种决策策略就叫做隐变量。在这个例子中，模型生成图像之前，首先采样了一个数字$z$，范围是$[0,...,9]$。我们不必知道潜变量的哪些设置生成了这个字符。

​	为了能表示整个数据集，那么对于数据集的每一个$X$，我们都要有一个潜变量$z$与之对应，使这个$z$能促进这个模型生成与$X$相似的东西。用正式符号表示：我们拥有一个潜变量$z$，其在的空间为高维空间$\mathcal{Z}$。我们可以通过一些PDF，$P(z) \ over\  \mathcal{Z} $来采样。接下来我们定义了一个决定函数$f(z;\theta)$，其是由参数空间$\Theta$的参数$\theta$参数化的。$f$把空间进行了转化即
$$
f: \mathcal{Z} \times \Theta \rightarrow \mathcal{X}
$$
我们希望学习$\theta$使$f(z;\theta)$能够通过采样$z$，生成出与数据集类似的图像。

​	即使下边的概率分布与数据集尽可能一致。
$$
P(X) = \int P(X|z;\theta)P(z)dz
$$
​	在这个该框架的背后思想是**极大似然估计**。

​	在VAE中，这个似然概率的分布往往是一个高斯分布
$$
P(X|z;\theta) = \mathcal{N}(X|f(z;\theta),\sigma^2I)
$$

## 2 VAE

​	VAE的目标就是极大似然估计，为了达到这一目标：VAE要解决两个问题，一个是如何定义潜变量$z$​，以及如何处理潜变量上的积分。

​	以手写数字为例，潜变量是很复杂的，如数字是多少，写数字的角度，线条的粗细，风格等等。更为恶劣的是，这些属性之间可能还有一些联系，如角度更大的数字是不是线条也会更细等等。因此VAE选取了一种不平常的手段，它不再手工设置这些潜变量，而是断言潜变量可以从简单分布中提取获得，如高斯分布，为什么可以这样呢？关键点在于，$d$维空间中的任何分布都可以通过采用一组正态分布的$d$维变量通过一个足够复杂的映射函数生成。例如：高斯分布变量生成一个环形分布。

![image-20240416152654586](https://raw.githubusercontent.com/Annzstbl/image-host/main/img/image-20240416152654586.png)

​	重新审视这个似然概率函数。
$$
P(X|z;\theta) = \mathcal{N}(X|f(z;\theta),\sigma^2I)
$$
如果$f(z;\theta)$是一个多层的神经网络，我们就可假设，这个网络的前几层把$z$映射到一个可解释的潜变量值，后几层则是把这个值映射到一个图像。一般来说，我们也不必纠结这样的潜变量值是否存在，只要这个似然概率可以使VAE的目标（极大似然估计）达到，它一定会在网络中的某几层学习到这样一种知识。

​	现在我们只需要极大似然估计即可。我们有
$$
P(z) = \mathcal{N}(Z|0, I)
$$
​	一个直观计算$P(X)$的方法是，取一组$z$，并对其取平均来代替积分
$$
P(X) \approx \frac1n\sum_iP(X|z_i)
$$
之后进行反向传播，进行优化。然而实际上这是非常困难的，其难在很难找到一个度量函数对图像的相似度进行度量。使大部分的$P(X|z_i)$​都是0。

## 2.1 Setting up the objective

​	取代上边的是，对于一个$X$，我们找到那些更可能有贡献的，或更可能出现的$z$。因此我们引入一个新的函数
$$
Q(z|X)
$$
这就使我们可以相对容易的计算
$$
E_{z \sim Q}P(X|z)
$$
从$Q(z)$而不是$\mathcal{N}(0, I)$取$z$有什么好处呢？我们考察$E_{z \sim Q}P(X|z)$和$P(X)$的关系。

​	对于任意的$Q$，我们有KL散度
$$
D[Q(z) || P(z|X)] = E_{z\sim Q}[\log Q(z) - \log P(z|X)]
$$
​	通过贝叶斯公式对$\log P(z|X)$分解，并且$Q$与$X$分布不相关
$$
D[Q(z) || P(z|X)] = E_{z\sim Q}[\log Q(z) - \log P(X|z) - \log P(z)] + \log P(X)
$$
​	移项，并且提取一个新的KL散度有：
$$
\log P(X) - D[Q(z) || P(z|X)] = E_{z\sim Q}[\log P(X|z)] - \mathcal D [Q(z)||P(z)]
$$
使$Q$确实和$X$相关，这样能使Q不是任意的分布，而是一个能够将$X$很好的映射到$z$的分布。
$$
\log P(X) - D[Q(z|X) || P(z|X)] = E_{z\sim Q}[\log P(X|z)] - \mathcal D [Q(z|X)||P(z)]
$$
​	这个式子的左边是我们要最大化的目标：$\log P(X)$，同时有一个误差项。这个误差项表示如果$Q$的品质越高，误差越小。

​	这个式子的右边是在给定Q的情况下，我们可以通过SGD方法进行优化的。

## 2.2 Optimizing the objective

​	首先$Q(z|X)$的形式需要去顶，一般的选择是高斯分布
$$
Q(z|X) = \mathcal N(z|\mu (X;\vartheta), \Sigma(X;\vartheta))
$$
​	两个函数$\mu \ and \ \Sigma$通常也是由$\vartheta$确定的神经网络。
